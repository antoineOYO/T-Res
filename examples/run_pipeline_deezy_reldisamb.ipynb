{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-RES using DeezyMatch with REL disambiguation\n",
    "\n",
    "REL disambiguation without filtering out microtoponyms and without adding place of publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "from t_res.geoparser import pipeline, ranking, linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Instantiate the ranker:\n",
    "myranker = ranking.Ranker(\n",
    "    method=\"deezymatch\",\n",
    "    resources_path=\"../resources/\",\n",
    "    mentions_to_wikidata=dict(),\n",
    "    wikidata_to_mentions=dict(),\n",
    "    strvar_parameters={\n",
    "        # Parameters to create the string pair dataset:\n",
    "        \"ocr_threshold\": 60,\n",
    "        \"top_threshold\": 85,\n",
    "        \"min_len\": 5,\n",
    "        \"max_len\": 15,\n",
    "        \"w2v_ocr_path\": str(Path(\"../resources/models/w2v/\").resolve()),\n",
    "        \"w2v_ocr_model\": \"w2v_*_news\",\n",
    "        \"overwrite_dataset\": False,\n",
    "    },\n",
    "    deezy_parameters={\n",
    "        # Paths and filenames of DeezyMatch models and data:\n",
    "        \"dm_path\": str(Path(\"../resources/deezymatch/\").resolve()),\n",
    "        \"dm_cands\": \"wkdtalts\",\n",
    "        \"dm_model\": \"w2v_ocr\",\n",
    "        \"dm_output\": \"deezymatch_on_the_fly\",\n",
    "        # Ranking measures:\n",
    "        \"ranking_metric\": \"faiss\",\n",
    "        \"selection_threshold\": 25,\n",
    "        \"num_candidates\": 3,\n",
    "        \"search_size\": 3,\n",
    "        \"verbose\": False,\n",
    "        # DeezyMatch training:\n",
    "        \"overwrite_training\": False,\n",
    "        \"do_test\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"../resources/rel_db/embeddings_database.db\") as conn:\n",
    "    cursor = conn.cursor()\n",
    "    mylinker = linking.Linker(\n",
    "        method=\"reldisamb\",\n",
    "        resources_path=\"../resources/\",\n",
    "        linking_resources=dict(),\n",
    "        rel_params={\n",
    "            \"model_path\": \"../resources/models/disambiguation/\",\n",
    "            \"data_path\": \"../experiments/outputs/data/lwm/\",\n",
    "            \"training_split\": \"originalsplit\",\n",
    "            \"context_length\": 100,\n",
    "            \"db_embeddings\": cursor,\n",
    "            \"with_publication\": False,\n",
    "            \"without_microtoponyms\": False,\n",
    "            \"do_test\": False,\n",
    "            \"default_publname\": \"\",\n",
    "            \"default_publwqid\": \"\",\n",
    "        },\n",
    "        overwrite_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Creating and loading a NER pipeline.\n",
      "*** Loading the ranker resources.\n",
      "The string match dataset already exists!\n",
      "\u001b[92m2024-03-28 16:45:21\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: /home/antoine/Documents/GitHub/T-Res/resources/deezymatch/inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:21\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:21\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: /home/antoine/Documents/GitHub/T-Res/resources/deezymatch/data/w2v_ocr_pairs.txt\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:25\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 610031 and False: 475483\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:25\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mSplitting the Dataset\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:25\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mfinish splitting the Dataset. User time: 0.3364291191101074\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:25\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msplits are as follow:\n",
      "train    922686\n",
      "val      162826\n",
      "test          2\n",
      "Name: split, dtype: int64\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:25\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mstart creating a lookup table and convert characters to indices\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:26\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create vocabulary\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:35\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- convert tokens to indices\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:35\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create a lookup table for tokens\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:35\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- read list of characters from ../resources/deezymatch/inputs/characters_v001.vocab\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:35\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- Length of vocabulary: 7554\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[92m2024-03-28 16:45:42\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:42\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m**** (Bi-directional) GRU ****\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:42\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:42\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread inputs\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:42\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mcreate a two_parallel_rnns model\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:42\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mstart fitting parameters\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:42\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of batches: 28834\u001b[0m\n",
      "\u001b[92m2024-03-28 16:45:42\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of epochs: 5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfdd9578b884bd38a194e7bb37d382a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e268bb08cb304638b2ac8c5e19f372bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "====================\n",
      "Total number of params: 627963\n",
      "\n",
      "two_parallel_rnns (\n",
      "  (emb): Embedding(7554, 60), weights=((7554, 60),), parameters=453240\n",
      "  (rnn_1): GRU(60, 60, num_layers=2, dropout=0.1, bidirectional=True), weights=((180, 60), (180, 60), (180,), (180,), (180, 60), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,)), parameters=109440\n",
      "  (attn_step1): Linear(in_features=120, out_features=60, bias=True), weights=((60, 120), (60,)), parameters=7260\n",
      "  (attn_step2): Linear(in_features=60, out_features=1, bias=True), weights=((1, 60), (1,)), parameters=61\n",
      "  (fc1): Linear(in_features=480, out_features=120, bias=True), weights=((120, 480), (120,)), parameters=57720\n",
      "  (fc2): Linear(in_features=120, out_features=2, bias=True), weights=((2, 120), (2,)), parameters=242\n",
      ")\n",
      "====================\n",
      "\n",
      "\n",
      "\u001b[92m2024-03-28 17:35:27\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m03/28/2024_17:35:27 -- Epoch: 1/5; Train; loss: 0.037; acc: 0.988; precision: 0.989, recall: 0.990, macrof1: 0.987, weightedf1: 0.988\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534d17df36674abfbde20f3ccf5e9cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5089 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2024-03-28 17:36:30\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m03/28/2024_17:36:30 -- Epoch: 1/5; Valid; loss: 0.019; acc: 0.993; precision: 0.987, recall: 1.000, macrof1: 0.992, weightedf1: 0.993\u001b[0m\n",
      "\u001b[92m2024-03-28 17:36:30\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c679863f69a429a8d2ee4d607de6bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2024-03-28 18:03:47\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m03/28/2024_18:03:47 -- Epoch: 2/5; Train; loss: 0.027; acc: 0.991; precision: 0.992, recall: 0.992, macrof1: 0.991, weightedf1: 0.991\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54617dbe25bf4e57ab5d99c90fb8b04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5089 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2024-03-28 18:04:56\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m03/28/2024_18:04:56 -- Epoch: 2/5; Valid; loss: 0.025; acc: 0.990; precision: 0.983, recall: 1.000, macrof1: 0.990, weightedf1: 0.990\u001b[0m\n",
      "\u001b[92m2024-03-28 18:04:56\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model (early stopped) with least valid loss (checkpoint: 1) at ../resources/deezymatch/models/w2v_ocr/w2v_ocr.model\u001b[0m\n",
      "\u001b[92m2024-03-28 18:04:56\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n",
      "\u001b[92m2024-03-28 18:04:56\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mEarly stopping at epoch: 2, selected epoch: 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "User time: 4753.9567\n",
      "====================\n",
      "\u001b[92m2024-03-28 18:04:59\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: /home/antoine/Documents/GitHub/T-Res/resources/deezymatch/models/w2v_ocr/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2024-03-28 18:05:00\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2024-03-28 18:05:00\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: /home/antoine/Documents/GitHub/T-Res/resources/deezymatch/data/wkdtalts.txt\u001b[0m\n",
      "\u001b[92m2024-03-28 18:05:04\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 1639965 and False: 0\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m geoparser \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmyranker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmylinker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmylinker\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/T-Res/t_res/geoparser/pipeline.py:139\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, myner, myranker, mylinker, resources_path, experiments_path)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyranker\u001b[38;5;241m.\u001b[39mmentions_to_wikidata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyranker\u001b[38;5;241m.\u001b[39mload_resources()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Train a DeezyMatch model if needed:\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmyranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# -----------------------------------------\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Linker loading resources:\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Load linking resources:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmylinker\u001b[38;5;241m.\u001b[39mlinking_resources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmylinker\u001b[38;5;241m.\u001b[39mload_resources()\n",
      "File \u001b[0;32m~/Documents/GitHub/T-Res/t_res/geoparser/ranking.py:291\u001b[0m, in \u001b[0;36mRanker.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeezy_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdm_cands\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     deezy_processing\u001b[38;5;241m.\u001b[39mtrain_deezy_model(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeezy_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrvar_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwikidata_to_mentions\n\u001b[1;32m    290\u001b[0m     )\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mdeezy_processing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeezy_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmentions_to_wikidata\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# This dictionary is not used anymore:\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwikidata_to_mentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/T-Res/t_res/utils/deezy_processing.py:397\u001b[0m, in \u001b[0;36mgenerate_candidates\u001b[0;34m(deezy_parameters, mentions_to_wikidata)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    387\u001b[0m     deezy_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_training\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m     )\u001b[38;5;241m.\u001b[39mis_dir()\n\u001b[1;32m    395\u001b[0m ):\n\u001b[1;32m    396\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 397\u001b[0m     \u001b[43mdm_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeezymatch_outputs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_dfm.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeezymatch_outputs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeezymatch_outputs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_vocab_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeezymatch_outputs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.vocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeezymatch_outputs_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcandidate_vectors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate candidate vectors: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m elapsed)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/t-res-rAxVKS4n-py3.9/lib/python3.9/site-packages/DeezyMatch/DeezyMatch.py:315\u001b[0m, in \u001b[0;36minference\u001b[0;34m(input_file_path, dataset_path, pretrained_model_path, pretrained_vocab_path, cutoff, inference_mode, scenario)\u001b[0m\n\u001b[1;32m    310\u001b[0m     pretrained_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    311\u001b[0m         pretrained_model_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpt_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# --- Inference / generate vector representations\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[43mrnn_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_vocab_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_vocab_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_cutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/t-res-rAxVKS4n-py3.9/lib/python3.9/site-packages/DeezyMatch/rnn_networks.py:1188\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model_path, dataset_path, train_vocab_path, input_file_path, test_cutoff, inference_mode, scenario, dl_inputs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     train_vocab \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(handle)\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# create the actual class here\u001b[39;00m\n\u001b[0;32m-> 1188\u001b[0m test_dc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_tokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmissing_char_threshold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreproc_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muni2ascii\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlowercase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monly_latin_letters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgru_lstm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_seq_len\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgru_lstm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_test_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_save_test_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv_sep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_column_inp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_column_inp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m test_dl \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m   1207\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dc, batch_size\u001b[38;5;241m=\u001b[39mdl_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgru_lstm\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m num_batch_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_dl)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/t-res-rAxVKS4n-py3.9/lib/python3.9/site-packages/DeezyMatch/data_processing.py:368\u001b[0m, in \u001b[0;36mtest_tokenize\u001b[0;34m(dataset_path, train_vocab, missing_char_threshold, preproc_steps, max_seq_len, mode, cutoff, save_test_class, dataframe_input, csv_sep, one_column_inp, verbose)\u001b[0m\n\u001b[1;32m    365\u001b[0m s2_tokenized \u001b[38;5;241m=\u001b[39m dataset_pd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms2_tokenized\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# XXX we need to explain why we have an if in the following for loop\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m s1_indx \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    369\u001b[0m     [train_vocab\u001b[38;5;241m.\u001b[39mtok2index[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m seq \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m train_vocab\u001b[38;5;241m.\u001b[39mtok2index]\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m s1_tokenized\n\u001b[1;32m    371\u001b[0m ]\n\u001b[1;32m    372\u001b[0m s2_indx \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    373\u001b[0m     [train_vocab\u001b[38;5;241m.\u001b[39mtok2index[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m seq \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m train_vocab\u001b[38;5;241m.\u001b[39mtok2index]\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m s2_tokenized\n\u001b[1;32m    375\u001b[0m ]\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Compute len(s1_indx) / len(s1_tokenized)\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# If this ratio is 1: all characters (after tokenization) could be found in the pretrained vocabulary\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Else: some characters are missing. If \"1 - (that ratio) > missing_char_threshold\", remove the entry\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/t-res-rAxVKS4n-py3.9/lib/python3.9/site-packages/DeezyMatch/data_processing.py:369\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    365\u001b[0m s2_tokenized \u001b[38;5;241m=\u001b[39m dataset_pd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms2_tokenized\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# XXX we need to explain why we have an if in the following for loop\u001b[39;00m\n\u001b[1;32m    368\u001b[0m s1_indx \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 369\u001b[0m     [train_vocab\u001b[38;5;241m.\u001b[39mtok2index[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m seq \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m train_vocab\u001b[38;5;241m.\u001b[39mtok2index]\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m s1_tokenized\n\u001b[1;32m    371\u001b[0m ]\n\u001b[1;32m    372\u001b[0m s2_indx \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    373\u001b[0m     [train_vocab\u001b[38;5;241m.\u001b[39mtok2index[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m seq \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m train_vocab\u001b[38;5;241m.\u001b[39mtok2index]\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m s2_tokenized\n\u001b[1;32m    375\u001b[0m ]\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Compute len(s1_indx) / len(s1_tokenized)\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# If this ratio is 1: all characters (after tokenization) could be found in the pretrained vocabulary\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Else: some characters are missing. If \"1 - (that ratio) > missing_char_threshold\", remove the entry\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/t-res-rAxVKS4n-py3.9/lib/python3.9/site-packages/DeezyMatch/data_processing.py:369\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    365\u001b[0m s2_tokenized \u001b[38;5;241m=\u001b[39m dataset_pd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms2_tokenized\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# XXX we need to explain why we have an if in the following for loop\u001b[39;00m\n\u001b[1;32m    368\u001b[0m s1_indx \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 369\u001b[0m     [train_vocab\u001b[38;5;241m.\u001b[39mtok2index[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m seq \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m train_vocab\u001b[38;5;241m.\u001b[39mtok2index]\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m s1_tokenized\n\u001b[1;32m    371\u001b[0m ]\n\u001b[1;32m    372\u001b[0m s2_indx \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    373\u001b[0m     [train_vocab\u001b[38;5;241m.\u001b[39mtok2index[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m seq \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m train_vocab\u001b[38;5;241m.\u001b[39mtok2index]\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m s2_tokenized\n\u001b[1;32m    375\u001b[0m ]\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Compute len(s1_indx) / len(s1_tokenized)\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# If this ratio is 1: all characters (after tokenization) could be found in the pretrained vocabulary\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Else: some characters are missing. If \"1 - (that ratio) > missing_char_threshold\", remove the entry\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "geoparser = pipeline.Pipeline(myranker=myranker, mylinker=mylinker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved = geoparser.run_text(\"A remarkable case of rattening has just occurred in the building trade at Shefrield, but also in Lancaster. Not in Nottingham though. Not in Ashton either, nor in Salop!\")\n",
    "    \n",
    "for r in resolved:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved = geoparser.run_sentence(\"A remarkable case of rattening has just occurred in the building trade at Sheffield.\")\n",
    "for r in resolved:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tres-turing",
   "language": "python",
   "name": "tres-turing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
