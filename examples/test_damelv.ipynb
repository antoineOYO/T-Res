{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WikidataObject as wdo\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "path = '/home/antoine/Documents/GitHub/T-Res/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987945\n"
     ]
    }
   ],
   "source": [
    "gaz = pd.read_csv(path + 'resources/wikidata/wikidata_gazetteer.csv', low_memory=False)\n",
    "gazetteer_ids = set(gaz.wikidata_id)\n",
    "print(len(gazetteer_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading the ranker resources.\n"
     ]
    }
   ],
   "source": [
    "def load_resources(method=\"mostpopular\",\n",
    "                   resources_path=\"../resources/\"\n",
    "                   ) :\n",
    "\n",
    "    print(\"*** Loading the ranker resources.\")\n",
    "\n",
    "    # Load files\n",
    "    files = {\n",
    "        \"mentions_to_wikidata\": os.path.join(\n",
    "            resources_path, \"wikidata/mentions_to_wikidata_normalized.json\"\n",
    "        ),\n",
    "        \"wikidata_to_mentions\": os.path.join(\n",
    "            resources_path, \"wikidata/wikidata_to_mentions_normalized.json\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    with open(files[\"mentions_to_wikidata\"], \"r\") as f:\n",
    "        mentions_to_wikidata = json.load(f)\n",
    "\n",
    "    with open(files[\"wikidata_to_mentions\"], \"r\") as f:\n",
    "        wikidata_to_mentions = json.load(f)\n",
    "\n",
    "    # Filter mentions to remove noise:\n",
    "    wikidata_to_mentions_filtered = dict()\n",
    "    mentions_to_wikidata_filtered = dict()\n",
    "    for wk in wikidata_to_mentions:\n",
    "        wikipedia_mentions = wikidata_to_mentions.get(wk)\n",
    "        wikipedia_mentions_stripped = dict(\n",
    "            [\n",
    "                (x, wikipedia_mentions[x])\n",
    "                for x in wikipedia_mentions\n",
    "                if not \", \" in x and not \" (\" in x\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if wikipedia_mentions_stripped:\n",
    "            wikipedia_mentions = wikipedia_mentions_stripped\n",
    "\n",
    "        wikidata_to_mentions_filtered[wk] = dict(\n",
    "            [(x, wikipedia_mentions[x]) for x in wikipedia_mentions]\n",
    "        )\n",
    "\n",
    "        for m in wikidata_to_mentions_filtered[wk]:\n",
    "            if m in mentions_to_wikidata_filtered:\n",
    "                mentions_to_wikidata_filtered[m][\n",
    "                    wk\n",
    "                ] = wikidata_to_mentions_filtered[wk][m]\n",
    "            else:\n",
    "                mentions_to_wikidata_filtered[m] = {\n",
    "                    wk: wikidata_to_mentions_filtered[wk][m]\n",
    "                }\n",
    "\n",
    "    mentions_to_wikidata = mentions_to_wikidata_filtered\n",
    "    wikidata_to_mentions = wikidata_to_mentions_filtered\n",
    "\n",
    "    del mentions_to_wikidata_filtered\n",
    "    del wikidata_to_mentions_filtered\n",
    "\n",
    "    # Parallelize if ranking method is one of the following:\n",
    "    if method in [\"partialmatch\", \"levenshtein\"]:\n",
    "        pandarallel.initialize(nb_workers=10)\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "    return mentions_to_wikidata, wikidata_to_mentions\n",
    "\n",
    "mentions_to_wikidata, wikidata_to_mentions = load_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests texte edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pyxdameraulevenshtein import normalized_damerau_levenshtein_distance as damlev\n",
    "from t_res.geoparser import geode_pipe,ranking,linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'VILLESFR.json'\n",
    "VILLESFR = pd.read_json(filepath, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myranker = ranking.Ranker(\n",
    "    method=\"levenshtein\",\n",
    "    resources_path=\"../resources/\",\n",
    ")\n",
    "mylinker = linking.Linker(\n",
    "    method=\"mostpopular\",\n",
    "    resources_path=\"../resources/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading the ranker resources.\n",
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "*** Load linking resources.\n",
      "  > Loading mentions to wikidata mapping.\n",
      "  > Loading gazetteer.\n",
      "*** Linking resources loaded!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geoparser = geode_pipe.Pipeline(geodeNERpath=NER_path,\n",
    "                                myranker=myranker,\n",
    "                                mylinker=mylinker,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27272728085517883"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damlev('ROQUEMADOUR'.lower(),\n",
    "       'Rocamadour'.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combien de candidats ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(path + 'examples/t_res_damlev+mostpop_170424.json',\n",
    "                   orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>head</th>\n",
       "      <th>fullcontent</th>\n",
       "      <th>gold</th>\n",
       "      <th>resolved</th>\n",
       "      <th>skyline</th>\n",
       "      <th>bestPred</th>\n",
       "      <th>acc10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>volume02-1574</td>\n",
       "      <td>BELLOC</td>\n",
       "      <td>* BELLOC , ( Géog . ) petite ville de France e...</td>\n",
       "      <td>Q369126</td>\n",
       "      <td>{'mention': 'BELLOC', 'ner_score': 1, 'pos': 2...</td>\n",
       "      <td>False</td>\n",
       "      <td>Q1017147</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               key    head                                        fullcontent  \\\n",
       "120  volume02-1574  BELLOC  * BELLOC , ( Géog . ) petite ville de France e...   \n",
       "\n",
       "        gold                                           resolved  skyline  \\\n",
       "120  Q369126  {'mention': 'BELLOC', 'ner_score': 1, 'pos': 2...    False   \n",
       "\n",
       "     bestPred  acc10  \n",
       "120  Q1017147  False  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ajout des alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish conexion with solr\n",
    "from pysolr import Solr\n",
    "solr = Solr('http://localhost:8983/solr/frenchtapioca5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query solr\n",
    "results = solr.search('id:Q90')\n",
    "'la ciutat de la llum' in results.docs[0]['mention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987945\n"
     ]
    }
   ],
   "source": [
    "gaz = pd.read_csv(path + 'resources/wikidata/wikidata_gazetteer.csv', low_memory=False)\n",
    "gazetteer_ids = set(gaz.wikidata_id)\n",
    "print(len(gazetteer_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wikidata_id', 'english_label', 'instance_of', 'alias_dict',\n",
       "       'nativelabel', 'hcounties', 'countries', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_labels_aliases(wd):\n",
    "\n",
    "    label_and_aliases = set()\n",
    "        \n",
    "    labels = wd._get_label(lang='all')\n",
    "    label_and_aliases.update(labels.values())\n",
    "\n",
    "    aliases = wd._get_aliases(lang='all')\n",
    "    for values in aliases.values():\n",
    "        label_and_aliases.update(v for v in values)\n",
    "\n",
    "    return list(label_and_aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the last saved JSON file if it exists\n",
    "def load_progress(filename):\n",
    "    json_file = path + 'resources/wikidata/{}.json'.format(filename)\n",
    "    if os.path.exists(json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# Function to dump the dict as a JSON file\n",
    "def save_progress(gaz_dict, filename):\n",
    "    with open(path + 'resources/wikidata/{}.json'.format(filename), 'w') as f:\n",
    "        json.dump(gaz_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987945/987945 [53:51<00:00, 305.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Request on Wikidata :  127510  \n",
      " Found in Solr :  860435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'gaz3'\n",
    "\n",
    "# Load the existing progress\n",
    "gaz_dict = load_progress(filename)\n",
    "\n",
    "# Initialize counters\n",
    "not_in_solr = 0\n",
    "in_solr = 0\n",
    "\n",
    "# Iterate over gaz dataframe\n",
    "for i, row in tqdm.tqdm(gaz.iterrows(), total=len(gaz)):\n",
    "    \n",
    "    if row[\"wikidata_id\"] in gaz_dict:\n",
    "        # If the entry already exists in gaz_dict, skip it\n",
    "        continue\n",
    "    \n",
    "    doc = solr.search('id:'+row[\"wikidata_id\"]).docs\n",
    "    if len(doc) == 0:\n",
    "        not_in_solr += 1\n",
    "        aka_list = []\n",
    "        # wd = wdo.WikidataObject(row[\"wikidata_id\"])\n",
    "        # aka_list = get_list_labels_aliases(wd)\n",
    "    else:\n",
    "        in_solr += 1\n",
    "        aka_list = doc[0]['mention']\n",
    "    \n",
    "    gaz_dict[row[\"wikidata_id\"]] = aka_list\n",
    "    \n",
    "    # Save progress every 1000 rows\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        save_progress(gaz_dict, filename)\n",
    "\n",
    "# Save final progress\n",
    "print(\"\\n Request on Wikidata : \", not_in_solr, \" \\n Found in Solr : \", in_solr, end='', flush=True)\n",
    "save_progress(gaz_dict, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_to_aka = dict()\n",
    "wikidata_to_aka_normalized = dict()\n",
    "aka_to_wikidata = dict()\n",
    "aka_to_wikidata_normalized = dict()\n",
    "\n",
    "for i,row in gaz_dict.items():\n",
    "\n",
    "    #qid = row[\"wikidata_id\"]\n",
    "    #akas = row[\"aka\"]\n",
    "    qid = i\n",
    "    akas = row\n",
    "    n = len(akas)\n",
    "\n",
    "    if not akas:\n",
    "        #\n",
    "        continue\n",
    "\n",
    "    wikidata_to_aka[qid] = {aka : 1 for aka in akas}\n",
    "    wikidata_to_aka_normalized[qid] = {aka: 1 for aka in akas}\n",
    "\n",
    "    for aka in akas:\n",
    "        if aka in aka_to_wikidata:\n",
    "            aka_to_wikidata[aka][qid] = 1\n",
    "            aka_to_wikidata_normalized[aka][qid] = 1\n",
    "        else:\n",
    "            aka_to_wikidata[aka] = {qid: 1}\n",
    "            aka_to_wikidata_normalized[aka] = {qid: 1}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860435"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikidata_to_aka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6141338"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aka_to_wikidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.7133996176352655"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_ambig = sum([len(akas) for akas in wikidata_to_aka.values()]) / len(wikidata_to_aka)\n",
    "mean_ambig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'resources/wikidata2/wikidata_to_mentions.json', 'w') as f:\n",
    "    json.dump(wikidata_to_aka, f)\n",
    "with open(path + 'resources/wikidata2/wikidata_to_mentions_normalized.json', 'w') as f:\n",
    "    json.dump(wikidata_to_aka_normalized, f)\n",
    "with open(path + 'resources/wikidata2/mentions_to_wikidata.json', 'w') as f:\n",
    "    json.dump(aka_to_wikidata, f)\n",
    "with open(path + 'resources/wikidata2/mentions_to_wikidata_normalized.json', 'w') as f:\n",
    "    json.dump(aka_to_wikidata_normalized, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging the gazeteer with the new aka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'resources/wikidata2/wikidata_to_mentions.json', 'r') as f:\n",
    "    wikidata_to_mentions = json.load(f)\n",
    "with open(path + 'resources/wikidata2/wikidata_to_mentions_normalized.json', 'r') as f:\n",
    "    wikidata_to_mentions_normalized = json.load(f)\n",
    "with open(path + 'resources/wikidata2/mentions_to_wikidata.json', 'r') as f:\n",
    "    mentions_to_wikidata = json.load(f)\n",
    "with open(path + 'resources/wikidata2/mentions_to_wikidata_normalized.json', 'r') as f:\n",
    "    mentions_to_wikidata_normalized = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Marseille' aka\n",
    "len(wikidata_to_mentions['Q23482'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wikidata_id', 'english_label', 'instance_of', 'alias_dict',\n",
       "       'nativelabel', 'hcounties', 'countries', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987945/987945 [00:31<00:00, 31367.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "gaz['aka'] = None\n",
    "for idx,row in tqdm.tqdm(gaz.iterrows(), total=len(gaz)):\n",
    "    if row[\"wikidata_id\"] in wikidata_to_mentions:\n",
    "        gaz.at[idx, 'aka'] = wikidata_to_mentions[row[\"wikidata_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidecoding the aka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"\n",
    "    Turn a Unicode string to plain ASCII, thanks to\n",
    "    https://stackoverflow.com/a/518232/2809427\n",
    "    >>>s1 = 'Санкт-Петербу́рг viertgrößte '\n",
    "    >>>'Санкт-Петербург viertgroßte'\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    \"\"\"\n",
    "    Lowercase, trim, and remove non-letter characters\n",
    "    >>>s1 = 'Francaise1 et Санкт-Петербург viertgroßte Europas 圣彼得堡 '\n",
    "    >>>'francaise et viertgro te europas'\n",
    "    \"\"\"\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "from unidecode import unidecode\n",
    "# 'Francaise1 et Санкт-Петербург viertgroßte Europas 圣彼得堡 '\n",
    "# 'Francaise1 et Sankt-Peterburg viertgrosste Europas Sheng Bi De Bao  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 114)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marseille_akas = wikidata_to_mentions['Q23482']\n",
    "unidecode_akas = dict({unidecode(aka):1 for aka in marseille_akas})\n",
    "len(marseille_akas), len(unidecode_akas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tres-turing",
   "language": "python",
   "name": "tres-turing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
