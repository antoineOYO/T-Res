{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and using a DeezyMatch model (option 2)\n",
    "\n",
    "This notebook shows how to train a new DeezyMatch model, given an existing string pairs dataset.\n",
    "\n",
    "To do so, the `resources/` folder should (at least) contain the following files, in the following locations:\n",
    "```\n",
    "toponym-resolution/\n",
    "   ├── ...\n",
    "   ├── resources/\n",
    "   │   ├── deezymatch/\n",
    "   │   │   ├── data/\n",
    "   │   │   │   └── w2v_ocr_pairs.txt\n",
    "   │   │   └── inputs/\n",
    "   │   │       ├── characters_v001.vocab\n",
    "   │   │       └── input_dfm.yaml\n",
    "   │   ├── models/\n",
    "   │   ├── news_datasets/\n",
    "   │   ├── wikidata/\n",
    "   │   │   └── mentions_to_wikidata.json\n",
    "   │   └── wikipedia/\n",
    "   └── ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing some libraries, and the `ranking` script from the `geoparser` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoine/.cache/pypoetry/virtualenvs/t-res-rAxVKS4n-py3.9/lib/python3.9/site-packages/DeezyMatch/rnn_networks.py:30: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from t_res.geoparser import ranking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `myranker` object of the `Ranker` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myranker = ranking.Ranker(\n",
    "    method=\"deezymatch\", # Here we're telling the ranker to use DeezyMatch.\n",
    "    resources_path=\"../resources/\", # Here, the path to the Wikidata resources.\n",
    "    # Parameters to create the string pair dataset:\n",
    "    strvar_parameters={\n",
    "        \"overwrite_dataset\": False,\n",
    "    },\n",
    "    # Parameters to train, load and use a DeezyMatch model:\n",
    "    deezy_parameters={\n",
    "        # Paths and filenames of DeezyMatch models and data:\n",
    "        \"dm_path\": str(Path(\"../resources/deezymatch/\").resolve()), # Path to the DeezyMatch directory where the model is saved.\n",
    "        \"dm_cands\": \"wkdtalts\", # Name we'll give to the folder that will contain the wikidata candidate vectors.\n",
    "        \"dm_model\": \"w2v_ocr\", # Name of the DeezyMatch model.\n",
    "        \"dm_output\": \"on_the_fly1\", # Name of the file where the output of DeezyMatch will be stored. Feel free to change that.\n",
    "        # Ranking measures:\n",
    "        \"ranking_metric\": \"faiss\", # Metric used by DeezyMatch to rank the candidates.\n",
    "        \"selection_threshold\": 50, # Threshold for that metric.\n",
    "        \"num_candidates\": 1, # Number of name variations for a string (e.g. \"London\", \"Londra\", and \"Londres\" are three different variations in our gazetteer of \"Londcn\").\n",
    "        \"verbose\": False, # Whether to see the DeezyMatch progress or not.\n",
    "        # DeezyMatch training:\n",
    "        \"overwrite_training\": True, # You can choose to overwrite the model if it exists: in this case we're loading an existing model, so that should be False.\n",
    "        \"do_test\": False, # Whether the DeezyMatch model we're loading was a test, or not.\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the resources (i.e. the `mentions-to-wikidata` and `wikidata-to-mentions` mappers) that will be used by the ranker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading the ranker resources.\n"
     ]
    }
   ],
   "source": [
    "# Load the resources:\n",
    "myranker.mentions_to_wikidata = myranker.load_resources()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a DeezyMatch model (notice we will be training a `test` model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string match dataset already exists!\n",
      "\u001b[92m2024-04-10 09:51:26\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: /home/antoine/Documents/GitHub/T-Res/resources/deezymatch/inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:26\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:26\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:26\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: /home/antoine/Documents/GitHub/T-Res/resources/deezymatch/data/w2v_ocr_pairs.txt\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:31\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 610031 and False: 475483\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:31\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mSplitting the Dataset\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:31\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mfinish splitting the Dataset. User time: 0.3357422351837158\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:31\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msplits are as follow:\n",
      "train    922686\n",
      "val      162826\n",
      "test          2\n",
      "Name: split, dtype: int64\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:31\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mstart creating a lookup table and convert characters to indices\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:32\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create vocabulary\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x76d90446e7f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/antoine/.cache/pypoetry/virtualenvs/t-res-rAxVKS4n-py3.9/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2024-04-10 09:51:40\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- convert tokens to indices\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:40\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create a lookup table for tokens\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:40\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- read list of characters from ../resources/deezymatch/inputs/characters_v001.vocab\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:40\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- Length of vocabulary: 7554\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[92m2024-04-10 09:51:47\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:47\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m**** (Bi-directional) GRU ****\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:47\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:47\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread inputs\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:47\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mcreate a two_parallel_rnns model\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:48\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mstart fitting parameters\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:48\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of batches: 28834\u001b[0m\n",
      "\u001b[92m2024-04-10 09:51:48\u001b[0m \u001b[95mantoine-liris\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of epochs: 5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8f4ebfac27489b91d2bfcc2eac23a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2d8ad3e56f4e5ebdbad720f94c23cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "====================\n",
      "Total number of params: 627963\n",
      "\n",
      "two_parallel_rnns (\n",
      "  (emb): Embedding(7554, 60), weights=((7554, 60),), parameters=453240\n",
      "  (rnn_1): GRU(60, 60, num_layers=2, dropout=0.1, bidirectional=True), weights=((180, 60), (180, 60), (180,), (180,), (180, 60), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,)), parameters=109440\n",
      "  (attn_step1): Linear(in_features=120, out_features=60, bias=True), weights=((60, 120), (60,)), parameters=7260\n",
      "  (attn_step2): Linear(in_features=60, out_features=1, bias=True), weights=((1, 60), (1,)), parameters=61\n",
      "  (fc1): Linear(in_features=480, out_features=120, bias=True), weights=((120, 480), (120,)), parameters=57720\n",
      "  (fc2): Linear(in_features=120, out_features=2, bias=True), weights=((2, 120), (2,)), parameters=242\n",
      ")\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a DeezyMatch model if needed:\n",
    "myranker.mentions_to_wikidata = myranker.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the DeezyMatch model that has been loaded, find candidates on Wikidata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find candidates given a toponym:\n",
    "toponym = \"Manchefter\"\n",
    "print(myranker.find_candidates([{\"mention\": toponym}])[toponym])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find candidates given a toponym:\n",
    "toponym = \"Londen\"\n",
    "print(myranker.find_candidates([{\"mention\": toponym}])[toponym])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tres-turing",
   "language": "python",
   "name": "tres-turing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
